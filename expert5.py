
### SLIP 1 ## =============================================================================================
# -*- coding: utf-8 -*-
"""Slip_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kgxf9tywTMoxs9ZEvJML8fZk7gZMu7Yn

Q. The credit.csv dataset classifies people described by a set of attributes as
good or bad credit risks. Develop a model to predict the credit risk based
on the dataset.
1. Perform exploratory data analysis and write the findings.
2. Find limitation with data if any for developing the model.
3. Develop the model using Decision Tree algorithm, support vector
machines and k nearest neighbors techniques.
4. Apply feature selection techniques and compare the performance.
5. Perform hyper parameter tuning
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('credit.csv')
df.head()

print(df.isnull().sum())

print(df.info())

df.duplicated().sum()

print(df.describe())

df.columns

#1. Perform remaining exploratory data analysis and write the findings

# 1. Check for outliers

plt.figure(figsize=(10, 6))
sns.boxplot(data=df)
plt.show()


non_numeric_cols = df.select_dtypes(include=['object', 'category']).columns
print(non_numeric_cols)


# Findings:

# - There are outliers in the 'income' and 'balance' columns.
# - There are strong positive correlations between 'income' and 'balance', and between 'income' and 'default'.
# - The target class is imbalanced, with more good credit risks than bad credit risks.

#2. Find limitation with data if any for developing the model.

# - The dataset is relatively small, with only 1000 data points. This could make it difficult to train a robust model.
# - The target class is imbalanced, which could also make it difficult to train a model that is accurate for both good and bad credit risks.
# - There are outliers in the 'income' and 'balance' columns, which could potentially affect the performance of the model.
# - The dataset does not contain any information about the applicants' credit history, which could be a useful feature for predicting credit risk.

#3. Import and Develop the model using Decision Tree algorithm, support vector machines and k nearest neighbors techniques.

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier


le = LabelEncoder()
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = le.fit_transform(df[col])


# Separate features and target
X = df.drop('class', axis=1)
y = df['class']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train models
dt_model = DecisionTreeClassifier()
svm_model = SVC()
knn_model = KNeighborsClassifier()

dt_model.fit(X_train, y_train)
svm_model.fit(X_train, y_train)
knn_model.fit(X_train, y_train)

# Evaluate models
dt_score = dt_model.score(X_test, y_test)
svm_score = svm_model.score(X_test, y_test)
knn_score = knn_model.score(X_test, y_test)

print(f'Decision Tree accuracy: {dt_score}')
print(f'SVM accuracy: {svm_score}')
print(f'KNN accuracy: {knn_score}')

"""5. Perform hyper parameter tuning"""

improve_dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=20)
improve_dt_model.fit(X_train, y_train)
improve_score= improve_dt_model.score(X_test, y_test)
print('Improved Decision Tree score:',improve_score)

improve_svm_model = SVC(random_state=42, C=10, kernel='rbf')
improve_svm_model.fit(X_train, y_train)
improve_score= improve_svm_model.score(X_test, y_test)
print('Improved SVM score:',improve_score)

improve_knn_model = KNeighborsClassifier(n_neighbors=2,weights='distance')
improve_knn_model.fit(X_train, y_train)
improve_score= improve_knn_model.score(X_test, y_test)
print('Improved KNN score:',improve_score)

 
# ## SLIP 2 ## =============================================================================================
# -*- coding: utf-8 -*-
"""Slip_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ao_d2oyujfZ0BceU82yGqd8Gom0-6pQT

Q1. The soybean.csv dataset contains 35 categorical attributes, some nominal
and some ordered. Develop a model to predict the class of soybean based on the
given attributes.
1. Find limitation with data if any in developing the model.
2. Develop the model to predict the soybean class using K nearest neighbors
algorithm.
3. Improve the model by changing the various parameters of the algorithm.
4. Evaluate the model.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df= pd.read_csv("/content/soybean.csv")
df.head()

"""**1. Find limitation with data if any in developing the model.**"""

print("Null values:",df.isnull().sum())

print("Duplicate values:",df.duplicated().sum())

df.info()

df.describe()

df.columns

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

for col in df.columns:
  df[col] = le.fit_transform(df[col])
df.head()

"""**2. Develop the model to predict the soybean class using K nearest neighbors algorithm.**"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

X= df.drop("class",axis=1)
y=df["class"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

KNN= KNeighborsClassifier()

KNN.fit(X_train,y_train)

y_pred = KNN.predict(X_test)

print("Prediction: ",y_pred)

"""**5. Evaluate the model.**"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

print("Accuracy score:",accuracy_score(y_test,y_pred))

print("Confusion matrix:")
print(confusion_matrix(y_test,y_pred))

print("Classification report:")
print(classification_report(y_test,y_pred))

"""**3. Improve the model by changing the various parameters of the algorithm.**"""

Improve_KNN= KNeighborsClassifier(n_neighbors=5,weights="distance",algorithm="auto")

Improve_KNN.fit(X_train,y_train)

y_pred= Improve_KNN.predict(X_test)

print("Improved Prediction: ",y_pred)

print("Accuracy score:",accuracy_score(y_test,y_pred))

print("Confusion matrix:")
print(confusion_matrix(y_test,y_pred))

print("Classification report:")
print(classification_report(y_test,y_pred))

"""Q2. Using linear regression algorithm to predict the medical insurance charges
based on the data given in med_insurance.csv.
1. Develop the model to predict the medical insurances charges.
2. Evaluate the model
"""

df= pd.read_csv("/content/med_insurance.csv")
df.head()

# Children
plt.figure(figsize=(3,3))
sns.countplot(x='children', data=df)
plt.title('Children')
plt.show()

# Smoker
plt.figure(figsize=(3,3))
sns.countplot(x='smoker', data=df)
plt.title('Smoker')
plt.show()

# Region
plt.figure(figsize=(3,3))
sns.countplot(x='region', data=df)
plt.title('Region')
plt.show()

df.columns

df.info()

df.isnull().sum()

for col in df.columns:
  df[col] = le.fit_transform(df[col])
df.head()

"""**1. Develop the model to predict the medical insurances charges.**"""

X= df.drop("charges",axis=1)
y=df["charges"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LinearRegression

LR= LinearRegression()

LR.fit(X_train,y_train)

y_pred = LR.predict(X_test)

print("Prediction: ",y_pred)

"""**2. Evaluate the model**"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

print("Accuracy score:",LR.score(X_test,y_test))

print("Mean squared error:",mean_squared_error(y_test,y_pred))

print("Mean absolute error:",mean_absolute_error(y_test,y_pred))

print("R2 score:",r2_score(y_test,y_pred))

### SLIP 3 ## =============================================================================================
# -*- coding: utf-8 -*-
"""Slip_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19cwIYUOJSYRMyzNQ1Czwnr3lMo-S7LKc

Q1. Use winequality.csv. Develop a model to predict the class of wine quality
based on the attributes given.
1. Find limitation with data if any in developing the model.
2. Develop the model to predict the quality of wine using Support Vector
Machines.
3. Try to improvise the model by changing the various parameters of the
algorithm.
4. Evaluate the model.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

wine_df= pd.read_csv('/content/winequality.csv')
wine_df.head()

"""**1. Find limitation with data if any in developing the model.**"""

wine_df.info()

wine_df.isnull().sum()

wine_df.describe()

wine_df.columns

X= wine_df.drop('quality', axis=1)
y= wine_df['quality']

"""**2. Develop the model to predict the quality of wine using Support Vector Machines.**"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)

svm_model= SVC()

svm_model.fit(X_train, y_train)

y_pred= svm_model.predict(X_test)

print("Prediction: ",y_pred)

print("Accuracy score: ",accuracy_score(y_pred, y_test))

"""**3. Try to improvise the model by changing the various parameters of the algorithm.**"""

improve_svm_model= SVC(kernel='linear', C=1.0, random_state=42, gamma='scale', probability=True)

improve_svm_model.fit(X_train, y_train)

y_pred= improve_svm_model.predict(X_test)

print("Improved Prediction: ",y_pred)

print("Improved accuracy score: ",accuracy_score(y_pred, y_test))

"""**5. Evaluate the model.**"""

from sklearn.metrics import classification_report, confusion_matrix

print("Confusion matrix: ")
print(confusion_matrix(y_test, y_pred))

print("Classification report: ")
print(classification_report(y_test, y_pred))

"""Q2. Use logistic regression algorithm to predict the iris flower species name
using iris.csv.
"""

iris_df= pd.read_csv('/content/iris.csv')
iris_df.head()

iris_df.info()

iris_df.isnull().sum()

iris_df.describe()

iris_df.columns

iris_df['Species']= iris_df['Class']

iris_df['Species'].unique()

X= iris_df.drop(['Class', 'Species'], axis=1)
y= iris_df['Species']

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)

LR= LogisticRegression()

LR.fit(X_train, y_train)

y_pred= LR.predict(X_test)

print("Prediction: ",y_pred)

print("Accuracy score: ", accuracy_score(y_pred, y_test))

print("Confusion matrix: ")
print(confusion_matrix(y_test, y_pred))

print("Classification report: ")
print(classification_report(y_test, y_pred))

### SLIP 4 ## =============================================================================================
# -*- coding: utf-8 -*-
"""Slip_4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9K0iiXhXUp3fooPyKwFp4O9McRMfCke

Q1. The digits.csv dataset consists of 16 input attributes to classify 10 digits.

1. Develop the model to predict the digits based on the features using
Random Forest classification algorithm.
2. Try to improve the model by using various values for the parameters.
3. Compare the performance of the model with ANN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

digits_df= pd.read_csv('/content/digits.csv')
digits_df.head()

digits_df.info()

digits_df.isnull().sum()

digits_df.describe()

digits_df.columns

X= digits_df.drop('class', axis=1)
y= digits_df['class']

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

RFC= RandomForestClassifier()

RFC.fit(X_train, y_train)

y_pred= RFC.predict(X_test)
print("Prediction: ", y_pred)

print("Accuracy: ", accuracy_score(y_test, y_pred))

print("Confusion matrix: ")
print(confusion_matrix(y_test, y_pred))

print("Classification report:")
print(classification_report(y_test, y_pred))

"""**2. Try to improve the model by using various values for the parameters.**"""

improve_RFC= RandomForestClassifier(n_estimators=300, max_depth=7, min_samples_split=20, random_state=42)

improve_RFC.fit(X_train, y_train)

y_pred= improve_RFC.predict(X_test)
print("Prediction: ", y_pred)

print("Accuracy: ", accuracy_score(y_test, y_pred))

"""**3. Compare the performance of the model with ANN**
- Not in our syllabus

Q2. Use mall_customers.csv to predict the spending score of the customers
based on gender, age and annual income.
"""

mall_df= pd.read_csv("/content/Mall_Customers.csv")
mall_df.head()

mall_df.info()

mall_df.isnull().sum()

mall_df.describe()

mall_df.columns

from sklearn.preprocessing import LabelEncoder

for col in mall_df.columns:
  if mall_df[col].dtype == 'object':
    le= LabelEncoder()
    mall_df[col]= le.fit_transform(mall_df[col])

X= mall_df.drop(['Spending Score (1-100)'], axis=1)
y= mall_df['Spending Score (1-100)']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LinearRegression

LR= LinearRegression()

LR.fit(X_train, y_train)

y_pred= LR.predict(X_test)
print("Prediction: ", y_pred)

from sklearn.metrics import r2_score

print("R2 score: ", r2_score(y_pred, y_test))

mall_df

prediction= LR.predict([[200,1,30,137]])
print("Random prediction: ",prediction)

### SLIP 5 ## =============================================================================================


# -*- coding: utf-8 -*-
"""Slip_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jw4NAKIrfzEY9L42ZtqHDKMyWFCb-bYH

Q1. Develop a model to determine which seed lots in a species are best for soil
conservation in seasonally dry hill country using eucalyptus.csv.

1. Analyse the Data Visually.
2. Find limitation with data if any in developing the model.
3. Develop a prediction model for quality of the seed lots using decision tree
classifier.
4. Compare the performance with support vector machines and naïve Bayes
Classifier.
5. Apply dimensionality reduction techniques.
6. Perform hyper parameter tuning and compare the performance.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df= pd.read_csv("/content/eucalyptus.csv")
df.head()

df.shape

df.info()

df.describe()

df.columns

"""**1. Analyse the Data Visually**"""

# Histogram
df.hist(figsize=(10,10))
plt.show()

# Visualize the data with boxplots
df.boxplot(figsize=(10, 5))
plt.title('Boxplot of Features')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

# Visualize the data with scatterplots
sns.scatterplot(data=df)
plt.title('Scatterplot of Features')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

from sklearn.preprocessing import LabelEncoder

le= LabelEncoder()

for col in df.columns:
  if df[col].dtype=='object':
    df[col]=le.fit_transform(df[col])
df.head()

#Data Correlation
df.corr()
sns.heatmap(df.corr(),annot=True)
plt.show()

"""**2. Find limitation with data if any in developing the model.**"""

# Check for missing values
missing_values = df.isnull().sum()
print("Missing values:", missing_values.sum())

# Check for duplicate rows
duplicate_rows = df[df.duplicated()]
print("Duplicate rows:", duplicate_rows.shape[0])

"""**3. Develop a prediction model for quality of the seed lots using decision tree classifier.**"""

df.columns

X= df.drop(['Utility'],axis=1)
y= df['Utility']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

dt_model= DecisionTreeClassifier(random_state=42)

dt_model.fit(X_train,y_train)

y_pred= dt_model.predict(X_test)
y_pred

dt_model= accuracy_score(y_test,y_pred)
dt_model

"""**4. Compare the performance with support vector machines and naïve Bayes Classifier.**


"""

from sklearn.svm import SVC

svm_model= SVC(kernel='poly')

svm_model.fit(X_train,y_train)

y_pred= svm_model.predict(X_test)
y_pred

svm_model= accuracy_score(y_test,y_pred)
svm_model

import sklearn.naive_bayes as BernoulliNB

nb_model= BernoulliNB.BernoulliNB()

nb_model.fit(X_train,y_train)

y_pred= nb_model.predict(X_test)
y_pred

nb_model= accuracy_score(y_test,y_pred)
nb_model

print(f"Decision Tree Accuracy: {dt_model}")
print(f"SVM Accuracy: {svm_model}")
print(f"Naive Bayes Accuracy: {nb_model}")

"""**5. Apply dimensionality reduction techniques.**
- Not in our syllabus

**6. Perform hyper parameter tuning and compare the performance.**
"""
